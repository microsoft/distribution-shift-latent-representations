{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "from typing import Callable, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from dslr.distance_utils import (\n",
    "    energy_distance, local_energy_distance, sliced_wasserstein_distance_persistence_features\n",
    ")\n",
    "from dslr.distribution_shift_utils import (\n",
    "    run_approximate_shift_test, wrap_subsample_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = '../embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = BASE + '/mnist/{}_embeddings.pkl'\n",
    "DATASET_NAMES = ['mnist_small', 'mnist_small_ds', 'mnist_small_sp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding data from blobstorage.\n",
    "def load_data():\n",
    "    data = {}\n",
    "    for dataset_name in DATASET_NAMES:\n",
    "        with open(TEMPLATE.format(dataset_name), 'rb') as f:\n",
    "            data[dataset_name] = pickle.load(f)\n",
    "        \n",
    "        # Print shapes.\n",
    "        for split_name in data[dataset_name].keys():\n",
    "            print(\n",
    "                dataset_name,\n",
    "                split_name,\n",
    "                data[dataset_name][split_name]['embeddings'].shape\n",
    "            )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Run Dirichlet Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation(\n",
    "    data_name: str,\n",
    "    shift_test: Callable,\n",
    "    distance_fn: Callable,\n",
    "    sample_sizes: List = [100],\n",
    "    repetitions: int = 1,\n",
    "    logfile: str = 'ablation_subsample.log'\n",
    ") -> dict:\n",
    "    \"\"\"Runs ablation on shift magnitude.\n",
    "    \n",
    "    Args:\n",
    "        data_name: Name of dataset to use.\n",
    "        shift_test: Function that runs shift test.\n",
    "        distance_fn: Distance function between two sets of embeddings.\n",
    "        sample_sizes: Sizes of subsamples to take in shift tests.\n",
    "        repetitions: Number of Dirichlet runs for a particular setting.\n",
    "        logfile: Log file for shift test.\n",
    "    \n",
    "    Returns:\n",
    "        results: Dict with sample size as key, and tuple of distances\n",
    "            and sensitivity proportions.\n",
    "    \"\"\"\n",
    "    data_train = np.array(DATA[data_name]['train']['embeddings'])\n",
    "    data_test = np.array(DATA[data_name]['test']['embeddings'])\n",
    "    labels_test = np.array(DATA[data_name]['test']['ids'])\n",
    "    \n",
    "    fig = plt.Figure()\n",
    "    colors = ['blue', 'green', 'red', 'gray', 'orange']\n",
    "    assert len(sample_sizes) <= len(colors)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Collect separate ablation results for each sample size.\n",
    "    for i, sample_size in enumerate(sample_sizes):\n",
    "        \n",
    "        distances = []\n",
    "        proportions = []\n",
    "\n",
    "        for k in range(repetitions):\n",
    "            # Sample new class weights.\n",
    "            # TODO: Choose alpha parameter to regulate variance in shifts.\n",
    "            alpha = np.logspace(-1.5, 4, num=repetitions)[k]\n",
    "            #alpha = 10\n",
    "            weights = np.random.dirichlet([alpha] * 10)\n",
    "\n",
    "            # Scale max weight to 1.0, while keep pairwise ratios.\n",
    "            weights_one_scaled = weights / max(weights)\n",
    "\n",
    "            # Aggregate indices for each class.\n",
    "            class_map = defaultdict(list)\n",
    "            for j, label in enumerate(labels_test):\n",
    "                class_map[label].append(j)\n",
    "\n",
    "            # Get indices per class based on weights.\n",
    "            new_indices = []\n",
    "            for target_class in class_map.keys():\n",
    "                target_indices = class_map[target_class]\n",
    "                np.random.shuffle(target_indices)\n",
    "                target_count = int(weights_one_scaled[target_class] * len(target_indices))\n",
    "                target_indices = target_indices[:target_count]\n",
    "                new_indices.extend(target_indices)\n",
    "\n",
    "            # Assign newly sampled data.\n",
    "            np.random.shuffle(new_indices)\n",
    "            data_test_reweighted = data_test[new_indices]\n",
    "            labels_test_reweighted = labels_test[new_indices]\n",
    "\n",
    "            # Compute distribution distance between datasets.\n",
    "            dist_xy = distance_fn(np.array(data_train), np.array(data_test_reweighted))\n",
    "\n",
    "            # Compute proportion of runs that detected a shift.\n",
    "            config = {\n",
    "                'dataset_name': data_name,\n",
    "                'data': {data_name: {'train': {'embeddings': data_train}, 'test': {'embeddings': data_test_reweighted}}},\n",
    "                'pair': ('train', 'test'),\n",
    "                'test_name': 'subsample',\n",
    "                'shift_test': shift_test,\n",
    "                'distance_measure': distance_fn,\n",
    "                'sample_size': sample_size,\n",
    "                'logfile': logfile,\n",
    "                'num_runs': 20\n",
    "            }\n",
    "            decision_counts, runs_dxx, runs_dxy, runs_pvals = run_approximate_shift_test(config)\n",
    "            proportion_detection = decision_counts[True] / sum(decision_counts.values())\n",
    "\n",
    "            # Add to collection of results.\n",
    "            distances.append(dist_xy)\n",
    "            proportions.append(proportion_detection)\n",
    "    \n",
    "        # Collect and plot results for this sample size.\n",
    "        plt.scatter(distances, proportions, c=colors[i], label=sample_size, alpha=0.35)\n",
    "        results[sample_size] = (distances, proportions)\n",
    "\n",
    "    \n",
    "    if distance_fn == local_energy_distance:\n",
    "        distance_str = 'local_energy_distance'\n",
    "    elif distance_fn == energy_distance:\n",
    "        distance_str = 'energy_distance'\n",
    "    elif distance_fn == sliced_wasserstein_distance_persistence_features:\n",
    "        distance_str = 'sliced_wasserstein_persistence'\n",
    "    \n",
    "    plt.title('Detection Sensitivity by Sample Size')\n",
    "    plt.xlabel(f'{distance_str}(X, Y)')\n",
    "    plt.ylabel('Positive Rate')\n",
    "    plt.legend()\n",
    "    filename = (\n",
    "        f'ablation_plot_{distance_str}_'\n",
    "        f'rep{str(repetitions)}_'\n",
    "        f'n{str(config[\"num_runs\"])}_'\n",
    "        f'ss{\"-\".join([str(n) for n in sample_sizes])}.alpharange.png'\n",
    "    )\n",
    "    plt.savefig(filename)\n",
    "    print(f'Saved output to {filename}')\n",
    "    plt.close()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'mnist_small'\n",
    "shift_test = wrap_subsample_test\n",
    "sample_sizes = [25, 50, 100]\n",
    "repetitions = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['text.usetex'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distance_fns = [local_energy_distance]\n",
    "for distance_fn in distance_fns:\n",
    "    run_ablation(data_name, shift_test, distance_fn, sample_sizes, repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dist_shift",
   "language": "python",
   "name": "dist_shift"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
